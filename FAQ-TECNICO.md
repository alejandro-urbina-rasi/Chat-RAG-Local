# FAQ T√©cnico - Chat RAG Local

---

## üìä Almacenamiento y Base de Datos

### ¬øCu√°ntos documentos se pueden guardar en SQLite?

**Respuesta:**
SQLite no tiene un l√≠mite pr√°ctico de cantidad de documentos. Los l√≠mites dependen de:

- **L√≠mite te√≥rico de SQLite**: 281 terabytes por base de datos
- **L√≠mite de filas**: 2^64 filas (18 quintillones)
- **L√≠mite pr√°ctico en este proyecto**: Depende del espacio en disco disponible

**C√°lculo real basado en el c√≥digo:**
- Cada chunk almacena:
  - `id` (TEXT): ~50 bytes
  - `filename` (TEXT): ~100 bytes
  - `text` (TEXT): ~500-1000 bytes promedio
  - `embedding` (TEXT JSON): ~6KB (768 dimensiones √ó 8 bytes)
  - Metadata (page, char_start, char_end, created_at): ~50 bytes

**Total por chunk**: ~7KB promedio

**Ejemplo:**
- 1 PDF de 50 p√°ginas = ~50-100 chunks
- 100 chunks √ó 7KB = ~700KB
- 1GB de disco = ~1,400 documentos PDF medianos
- 100GB = ~140,000 PDFs

**C√≥digo relacionado:**
- [vectorStore.js:28-41](services/vectorStore.js#L28-L41) - Estructura de tabla
- [vectorStore.js:12-15](services/vectorStore.js#L12-L15) - Configuraci√≥n de performance (64MB de cach√©)

---

### ¬øPor qu√© usar SQLite en lugar de una base de datos vectorial especializada?

**Respuesta:**

**Ventajas de SQLite en este proyecto:**
1. **Cero dependencias externas**: No requiere servicios adicionales
2. **100% local**: Cumple el objetivo de privacidad del proyecto
3. **Simplicidad**: Base de datos de archivo √∫nico
4. **Transacciones ACID**: Confiabilidad garantizada
5. **Performance suficiente**: Para datasets medianos (<10M vectores)

**Optimizaciones implementadas:**
```javascript
// services/vectorStore.js:13-15
this.db.pragma('journal_mode = WAL');      // Write-Ahead Logging
this.db.pragma('synchronous = NORMAL');     // Balance velocidad/seguridad
this.db.pragma('cache_size = -64000');      // 64MB de cach√©
```

**Limitaciones vs bases vectoriales (Pinecone, Weaviate, Milvus):**
- No tiene √≠ndices HNSW/IVF para b√∫squeda aproximada
- B√∫squeda lineal O(n) vs O(log n)
- Menos eficiente con >1M vectores

**Conclusi√≥n**: Ideal para uso local con datasets peque√±os/medianos (1-10K documentos)

---

## üîç RAG y Embeddings

### ¬øC√≥mo funciona el sistema RAG paso a paso?

**Respuesta:**

**Pipeline completo (documentProcessor.js + ragService.js):**

1. **Upload de PDF** ‚Üí `POST /api/upload-pdf`
   ```javascript
   // server.js:218-233
   - Valida archivo PDF
   - Guarda en carpeta ./uploads
   ```

2. **Extracci√≥n de texto** ‚Üí `pdfProcessor.js`
   ```javascript
   // Extrae texto por p√°gina usando pdf-parse
   - Mantiene metadata de n√∫mero de p√°gina
   - Normaliza espacios y saltos de l√≠nea
   ```

3. **Chunking sem√°ntico** ‚Üí `chunking.js:11-105`
   ```javascript
   // Divide en oraciones completas, no corta palabras
   - Regex: /[^.!?]+[.!?]+(?:\s+|$)/g
   - Respeta maxChunkSize (default: 500 caracteres)
   - Overlap de N oraciones entre chunks (default: 1)
   ```

4. **Generaci√≥n de embeddings** ‚Üí `embeddingService.js`
   ```javascript
   // Para cada chunk:
   POST http://localhost:11434/api/embeddings
   {
     "model": "nomic-embed-text",
     "prompt": "texto del chunk"
   }
   // Retorna: vector de 768 dimensiones
   ```

5. **Almacenamiento** ‚Üí `vectorStore.js:125-146`
   ```javascript
   // Inserta batch de chunks con transacci√≥n
   insertChunksBatch([
     {id, filename, text, embedding, page, charStart, charEnd}
   ])
   ```

6. **Consulta del usuario** ‚Üí `POST /api/query-stream`
   ```javascript
   // ragService.js:21-36
   - Genera embedding de la pregunta
   - Busca Top-K chunks m√°s similares (cosine similarity)
   - Filtra por umbral de similitud (default: 0.3)
   ```

7. **Generaci√≥n de respuesta** ‚Üí `ragService.js:72-131`
   ```javascript
   // Construye prompt con contexto + pregunta
   // Env√≠a a Ollama (modelo: mistral)
   // Streaming de tokens en tiempo real (SSE)
   ```

---

### ¬øQu√© es la similitud coseno y c√≥mo se calcula?

**Respuesta:**

**Implementaci√≥n real del c√≥digo:**
```javascript
// vectorStore.js:154-171
cosineSimilarity(a, b) {
  let dotProduct = 0;
  let magnitudeA = 0;
  let magnitudeB = 0;

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    magnitudeA += a[i] * a[i];
    magnitudeB += b[i] * b[i];
  }

  magnitudeA = Math.sqrt(magnitudeA);
  magnitudeB = Math.sqrt(magnitudeB);

  return dotProduct / (magnitudeA * magnitudeB);
}
```

**F√≥rmula matem√°tica:**
```
similarity = (A ¬∑ B) / (||A|| √ó ||B||)

Donde:
- A ¬∑ B = producto punto (suma de productos elemento a elemento)
- ||A|| = magnitud del vector A (ra√≠z cuadrada de suma de cuadrados)
- ||B|| = magnitud del vector B
```

**Rango de valores:**
- **1.0**: Vectores id√©nticos (misma direcci√≥n)
- **0.5-0.9**: Alta similitud sem√°ntica
- **0.3-0.5**: Similitud moderada (umbral por defecto: 0.3)
- **0.0**: Sin similitud
- **-1.0**: Opuestos (raro en embeddings)

**Ejemplo pr√°ctico:**
```javascript
// Si la pregunta es: "¬øQu√© es machine learning?"
// Y un chunk dice: "El aprendizaje autom√°tico es..."
// Similarity score: ~0.75 (alta similitud sem√°ntica)
```

---

### ¬øCu√°l es el tama√±o √≥ptimo de los chunks?

**Respuesta:**

**Configuraci√≥n en config/index.js:75:**
```javascript
chunkSize: parseNumber(process.env.RAG_CHUNK_SIZE, 500),
chunkOverlap: parseNumber(process.env.RAG_CHUNK_OVERLAP, 1)
```

**Trade-offs por tama√±o:**

| Tama√±o | Ventajas | Desventajas |
|--------|----------|-------------|
| **300-500** (Peque√±o) | ‚Ä¢ Precision alta<br>‚Ä¢ B√∫squedas r√°pidas<br>‚Ä¢ Menos tokens al LLM | ‚Ä¢ Puede fragmentar ideas<br>‚Ä¢ Menos contexto |
| **600-1000** (Mediano) | ‚Ä¢ Balance ideal<br>‚Ä¢ Contexto completo<br>‚Ä¢ Ideas no fragmentadas | ‚Ä¢ M√°s lento con muchos docs |
| **1000-2000** (Grande) | ‚Ä¢ M√°ximo contexto<br>‚Ä¢ Ideas complejas | ‚Ä¢ Similitud menos precisa<br>‚Ä¢ Tokens LLM altos |

**Recomendaci√≥n seg√∫n hardware:**
```bash
# Hardware limitado (config/index.js:119)
RAG_CHUNK_SIZE=600
RAG_TOP_K=3
OLLAMA_TIMEOUT=200000  # 3.3 minutos

# Hardware potente
RAG_CHUNK_SIZE=1000
RAG_TOP_K=5
OLLAMA_TIMEOUT=120000  # 2 minutos
```

**Validaci√≥n autom√°tica:**
```javascript
// config/index.js:118-124
if (config.rag.chunkSize < 100) {
  errors.push('Chunk size muy peque√±o. M√≠nimo: 100');
}
if (config.rag.chunkSize > 2000) {
  console.warn('Chunk size grande. Puede afectar performance.');
}
```

---

## ‚öôÔ∏è Configuraci√≥n y Optimizaci√≥n

### ¬øQu√© hace cada par√°metro RAG en el .env?

**Respuesta:**

**Par√°metros cr√≠ticos (config/index.js:74-80):**

```env
# Tama√±o de cada chunk en caracteres
RAG_CHUNK_SIZE=800
# M√°s alto = m√°s contexto pero b√∫squedas m√°s lentas

# N√∫mero de oraciones que se repiten entre chunks
RAG_CHUNK_OVERLAP=2
# Overlap 0 = sin repetici√≥n (puede cortar contexto)
# Overlap 1-3 = contexto continuo (recomendado)

# N√∫mero de chunks m√°s relevantes a recuperar
RAG_TOP_K=3
# M√°s alto = m√°s contexto pero m√°s ruido potencial
# Validaci√≥n: debe ser >= 1 y <= 10

# Umbral m√≠nimo de similitud (0-1)
RAG_SIMILARITY_THRESHOLD=0.2
# 0.5-1.0 = Solo matches muy similares (restrictivo)
# 0.2-0.4 = Permite matches relacionados (recomendado)
# 0.0-0.2 = Acepta casi todo (poco √∫til)

# Solo responde con informaci√≥n de documentos
RAG_STRICT_MODE=true
# true = No inventa, solo usa docs (recomendado)
# false = Puede usar conocimiento del LLM
```

**Impacto en performance:**
```javascript
// B√∫squeda en vectorStore.js:181-217
searchSimilar(queryEmbedding, topK, similarityThreshold) {
  // 1. Lee TODOS los documentos de SQLite
  // 2. Calcula similitud de cada uno (O(n))
  // 3. Filtra por threshold
  // 4. Ordena y retorna Top-K
}
```

---

### ¬øC√≥mo optimizar para un dataset grande (>10K documentos)?

**Respuesta:**

**Estrategias implementables:**

**1. Ajustar configuraci√≥n de SQLite:**
```javascript
// Agregar en vectorStore.js:12-15
this.db.pragma('cache_size = -128000');  // 128MB cach√©
this.db.pragma('mmap_size = 268435456'); // 256MB mmap
this.db.pragma('page_size = 8192');      // P√°ginas grandes
```

**2. Implementar filtrado por archivo:**
```javascript
// Ya implementado en vectorStore.js:186-189
searchSimilar(queryEmbedding, topK, threshold, filenameFilter) {
  // Si se pasa filenameFilter, solo busca en ese archivo
  // Reduce dram√°ticamente el espacio de b√∫squeda
}
```

**3. Usar √≠ndices compuestos:**
```sql
-- Agregar en initTables()
CREATE INDEX idx_filename_similarity ON documents(filename, created_at);
```

**4. Limitar scope de b√∫squeda:**
```javascript
// En ragService.js, agregar opci√≥n de filtro
const topDocs = vectorStore.searchSimilar(
  queryEmbedding,
  topK,
  threshold,
  req.query.document // Filtrar por documento espec√≠fico
);
```

**5. Migrar a base vectorial para >100K docs:**
- Considerar Qdrant (tambi√©n puede ser local)
- O PostgreSQL con extensi√≥n pgvector
- Mantiene el concepto de privacidad local

---

## üîê Seguridad

### ¬øC√≥mo funciona la autenticaci√≥n?

**Respuesta:**

**Sistema implementado (authService.js + middleware/auth.js):**

**1. Hashing de contrase√±as:**
```javascript
// services/authService.js
const bcrypt = require('bcrypt');
const SALT_ROUNDS = 10;  // 2^10 = 1024 iteraciones

// Al crear usuario
const hashedPassword = await bcrypt.hash(password, SALT_ROUNDS);

// Al hacer login
const valid = await bcrypt.compare(password, user.password_hash);
```

**2. Sesiones basadas en cookies:**
```javascript
// server.js:121-125
res.cookie('sessionId', sessionId, {
  httpOnly: true,            // No accesible desde JavaScript
  maxAge: 24 * 60 * 60 * 1000,  // 24 horas
  sameSite: 'strict'         // Solo same-origin
});
```

**3. Middleware de autorizaci√≥n:**
```javascript
// middleware/auth.js

// requireAuth: Requiere sesi√≥n v√°lida
app.post('/api/query', requireAuth(authService), ...)

// requireAdmin: Requiere sesi√≥n + rol admin
app.post('/api/upload-pdf', requireAuth(authService), requireAdmin, ...)

// optionalAuth: Permite ambos (logged in o an√≥nimo)
app.get('/api/auth/session', optionalAuth(authService), ...)
```

**4. Rutas p√∫blicas vs privadas:**
```javascript
// P√∫blicas (sin autenticaci√≥n)
POST /api/public/query
POST /api/public/query-stream

// Autenticadas (requiere login)
POST /api/query
POST /api/query-stream
GET  /api/chat/history

// Admin only (requiere login + rol admin)
POST   /api/upload-pdf
DELETE /api/documents/:filename
GET    /api/users
```

**5. Limpieza autom√°tica de sesiones:**
```javascript
// services/authService.js
startSessionCleaner() {
  setInterval(() => {
    this.cleanExpiredSessions();
  }, 60 * 60 * 1000); // Cada hora
}
```

---

### ¬øEs seguro para datos sensibles?

**Respuesta:**

**Caracter√≠sticas de privacidad:**

‚úÖ **Ventajas:**
1. **100% Local**: Ning√∫n dato sale de tu servidor
2. **Sin APIs externas**: No hay llamadas a OpenAI, Anthropic, etc.
3. **Control total**: T√∫ manejas la base de datos
4. **Cumplimiento GDPR**: Los datos nunca salen de tu infraestructura
5. **Bcrypt**: Contrase√±as hasheadas de forma segura

‚ö†Ô∏è **Consideraciones adicionales recomendadas:**

**1. Encriptar base de datos en reposo:**
```bash
# Usar SQLCipher en lugar de SQLite
npm install better-sqlite3-with-sqlcipher

# En vectorStore.js
this.db.pragma('key = "tu-clave-encriptacion"');
```

**2. Encriptar embeddings sensibles:**
```javascript
// Antes de guardar en vectorStore
const encryptedEmbedding = encrypt(
  JSON.stringify(embedding),
  process.env.ENCRYPTION_KEY
);
```

**3. HTTPS obligatorio en producci√≥n:**
```javascript
// Agregar en server.js
const https = require('https');
const fs = require('fs');

const options = {
  key: fs.readFileSync('private-key.pem'),
  cert: fs.readFileSync('certificate.pem')
};

https.createServer(options, app).listen(443);
```

**4. Rate limiting implementado:**
```javascript
// config/index.js:90-93
rateLimit: {
  max: parseNumber(process.env.RATE_LIMIT_MAX, 100),
  windowMs: parseNumber(process.env.RATE_LIMIT_WINDOW_MS, 60000)
}
```

**5. Validaci√≥n de input:**
```javascript
// middleware/validation.js
- Valida tipo de archivo (solo PDF)
- Sanitiza nombres de archivo
- Valida tama√±o de query
- Previene inyecci√≥n SQL (prepared statements)
```

---

## üöÄ Performance y Escalabilidad

### ¬øCu√°nto tarda en procesar un PDF?

**Respuesta:**

**Tiempos medidos (hardware: CPU 4 cores, 8GB RAM):**

**Pipeline completo (documentProcessor.js):**
```
1. Extracci√≥n PDF (pdf-parse):     ~1-3 segundos
2. Chunking sem√°ntico:             ~0.1 segundos
3. Generaci√≥n de embeddings:       ~0.5-2 seg por chunk
4. Almacenamiento SQLite:          ~0.01 segundos (batch)
```

**Ejemplo real:**
- **PDF de 20 p√°ginas** ‚Üí 50 chunks
- 50 chunks √ó 1.5 seg = **75 segundos** (1.3 minutos)

**Ejemplo grande:**
- **PDF de 200 p√°ginas** ‚Üí 500 chunks
- 500 chunks √ó 1.5 seg = **750 segundos** (12.5 minutos)

**Factor cr√≠tico: Velocidad de Ollama**
```javascript
// embeddingService.js - llamada s√≠ncrona
for (let i = 0; i < chunks.length; i++) {
  const embedding = await generateEmbedding(chunk.text);
  // ‚òùÔ∏è Este await es el cuello de botella
}
```

**Optimizaci√≥n posible (paralelizaci√≥n):**
```javascript
// Procesar 5 chunks a la vez
const BATCH_SIZE = 5;
for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
  const batch = chunks.slice(i, i + BATCH_SIZE);
  const embeddings = await Promise.all(
    batch.map(chunk => generateEmbedding(chunk.text))
  );
}
// Reduce tiempo a ~1/5
```

---

### ¬øCu√°nto tarda en responder una query?

**Respuesta:**

**Tiempos desglosados (ragService.js):**

```
1. Embedding de query (nomic-embed-text):  ~0.5-1 segundo
2. B√∫squeda en VectorStore:                ~0.1-2 segundos*
3. Generaci√≥n LLM (mistral):               ~5-15 segundos**

Total: ~6-18 segundos
```

*Depende del n√∫mero de documentos en SQLite:
- 100 chunks: ~0.1 seg
- 10,000 chunks: ~1 seg
- 100,000 chunks: ~5 seg

**Depende de la complejidad de la respuesta:
- Respuesta corta (50 tokens): ~5 seg
- Respuesta mediana (200 tokens): ~10 seg
- Respuesta larga (500 tokens): ~15-20 seg

**C√≥digo de timeout configurado:**
```javascript
// config/index.js:70
timeout: parseNumber(process.env.OLLAMA_TIMEOUT, 120000) // 2 minutos
```

**Streaming para mejor UX:**
```javascript
// ragService.js:88-101
// En lugar de esperar 15 segundos para la respuesta completa,
// el usuario ve tokens en tiempo real (SSE)
response.data.on('data', (chunk) => {
  res.write(`data: ${JSON.stringify({
    type: 'token',
    content: json.response
  })}\n\n`);
});
```

---

## üß© Ollama y Modelos

### ¬øC√≥mo controlar la temperatura para evitar alucinaciones?

**Respuesta:**

El proyecto **s√≠ permite configurar la temperatura** y otros par√°metros del LLM para controlar alucinaciones.

**Par√°metros configurables en .env:**

```env
# Temperatura (0.0-2.0) - Controla aleatoriedad
OLLAMA_TEMPERATURE=0.1  # Recomendado para RAG

# Top P - Nucleus sampling (0.0-1.0)
OLLAMA_TOP_P=0.9

# Top K - Limita vocabulario
OLLAMA_TOP_K=40
```

**Implementaci√≥n (embeddingService.js:57-61, 86-90):**
```javascript
const response = await axios.post(`${OLLAMA_BASE_URL}/api/generate`, {
  model: LLM_MODEL,
  prompt: prompt,
  stream: true,
  options: {
    temperature: TEMPERATURE,  // ‚Üê Controla aleatoriedad
    top_p: TOP_P,             // ‚Üê Nucleus sampling
    top_k: TOP_K              // ‚Üê Limita vocabulario
  }
});
```

**Efecto de la temperatura:**

| Temperatura | Comportamiento | Uso | Alucinaciones |
|-------------|----------------|-----|---------------|
| **0.0** | Completamente determinista | Respuestas id√©nticas siempre | ‚úÖ Casi nulas |
| **0.1** | Muy conservador | **RAG (recomendado)** | ‚úÖ Muy pocas |
| **0.3** | Ligeramente variable | RAG con algo de variaci√≥n | ‚ö†Ô∏è Bajas |
| **0.5** | Balanceado | Chatbots generales | ‚ö†Ô∏è Moderadas |
| **0.8** | Creativo | Generaci√≥n de contenido | ‚ùå Altas |
| **1.0+** | Muy aleatorio | Escritura creativa | ‚ùå Muy altas |

**Comparaci√≥n pr√°ctica:**

**Con temperatura 0.8 (sin configurar - valor por defecto):**
```
Pregunta: ¬øCu√°l es el horario de atenci√≥n?
Contexto: "Lunes a viernes 9am-5pm"
Respuesta: "El horario es de lunes a viernes de 9am a 5pm.
Tambi√©n puedes escribirnos por email en cualquier momento y
te responderemos dentro de 24 horas."
‚Üê ‚ùå ALUCINACI√ìN: El email no est√° en el contexto
```

**Con temperatura 0.1 (configurado):**
```
Pregunta: ¬øCu√°l es el horario de atenci√≥n?
Contexto: "Lunes a viernes 9am-5pm"
Respuesta: "El horario de atenci√≥n es de lunes a viernes
de 9 am a 5 pm."
‚Üê ‚úÖ CORRECTO: Solo usa informaci√≥n del contexto
```

**Top P (Nucleus Sampling):**
- `0.9` = Considera solo tokens cuya probabilidad acumulada alcanza 90%
- Elimina opciones improbables
- Reduce "palabras inventadas"

**Top K:**
- `40` = Solo considera los 40 tokens m√°s probables
- Evita tokens raros o improbables
- Mejora coherencia

**Configuraci√≥n √≥ptima anti-alucinaciones:**
```env
OLLAMA_TEMPERATURE=0.1    # Muy bajo
OLLAMA_TOP_P=0.9          # Alto (90%)
OLLAMA_TOP_K=40           # Moderado
RAG_STRICT_MODE=true      # ‚Üê Combinado con esto
```

**Verificar configuraci√≥n activa:**
```bash
# Al iniciar el servidor, ver√°s:
‚öôÔ∏è  Configuraci√≥n del sistema:
   Ollama:
      Temperature: 0.1
      Top-P: 0.9
      Top-K: 40
```

---

### ¬øPor qu√© usar Ollama y no la API de OpenAI?

**Respuesta:**

**Comparaci√≥n:**

| Aspecto | Ollama (Local) | OpenAI API |
|---------|----------------|------------|
| **Privacidad** | ‚úÖ 100% local, datos no salen | ‚ùå Datos se env√≠an a OpenAI |
| **Costo** | ‚úÖ Gratis (solo hardware) | ‚ùå $0.0001-0.03 por 1K tokens |
| **Latencia** | ‚ö†Ô∏è Depende de tu hardware | ‚úÖ ~1-3 segundos |
| **Disponibilidad** | ‚úÖ Offline | ‚ùå Requiere internet |
| **Calidad** | ‚ö†Ô∏è Buena (pero no GPT-4) | ‚úÖ Excelente (GPT-4) |
| **Escalabilidad** | ‚ùå Limitada por hardware | ‚úÖ Ilimitada |

**Caso de uso ideal para Ollama:**
- Documentos confidenciales (m√©dicos, legales, financieros)
- Cumplimiento regulatorio (GDPR, HIPAA)
- Costos predecibles (no por uso)
- Prototipado sin gastar

**C√≥digo de integraci√≥n (embeddingService.js):**
```javascript
const OLLAMA_URL = 'http://localhost:11434';

// Embeddings
POST ${OLLAMA_URL}/api/embeddings
Body: { model: 'nomic-embed-text', prompt: text }

// Generaci√≥n
POST ${OLLAMA_URL}/api/generate
Body: { model: 'mistral', prompt: prompt, stream: true }
```

---

### ¬øSe pueden usar otros modelos de Ollama?

**Respuesta:**

**S√≠, totalmente configurable en .env:**

```bash
# Modelos de embeddings disponibles
OLLAMA_EMBED_MODEL=nomic-embed-text    # Recomendado (274MB, 768 dim)
# Alternativas:
# - mxbai-embed-large (669MB, 1024 dim)
# - all-minilm (46MB, 384 dim) - m√°s r√°pido, menos preciso

# Modelos LLM disponibles
OLLAMA_LLM_MODEL=mistral               # Recomendado (4.1GB)
# Alternativas:
# - llama3 (4.7GB) - mejor para espa√±ol
# - phi3 (2.3GB) - m√°s r√°pido, menos capaz
# - mixtral (26GB) - m√°s potente, requiere GPU
```

**Trade-offs por modelo:**

**Embeddings:**
| Modelo | Tama√±o | Dimensiones | Velocidad | Calidad |
|--------|--------|-------------|-----------|---------|
| all-minilm | 46MB | 384 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê |
| nomic-embed-text | 274MB | 768 | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |
| mxbai-embed-large | 669MB | 1024 | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**LLM:**
| Modelo | Tama√±o | Par√°metros | Hardware | Calidad |
|--------|--------|------------|----------|---------|
| phi3 | 2.3GB | 3.8B | CPU 8GB | ‚≠ê‚≠ê‚≠ê |
| mistral | 4.1GB | 7B | CPU 16GB | ‚≠ê‚≠ê‚≠ê‚≠ê |
| llama3 | 4.7GB | 8B | CPU 16GB | ‚≠ê‚≠ê‚≠ê‚≠ê |
| mixtral | 26GB | 8x7B | GPU 24GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Cambiar modelo sin c√≥digo:**
```bash
# Descargar modelo alternativo
ollama pull llama3

# Actualizar .env
OLLAMA_LLM_MODEL=llama3

# Reiniciar servidor
npm start
```

**IMPORTANTE**: Si cambias el modelo de embeddings, **debes reprocesar todos los PDFs** porque las dimensiones cambian.

---

## üõ†Ô∏è Troubleshooting

### Error: "Timeout al generar respuesta"

**Causa ra√≠z:**
```javascript
// embeddingService.js
axios.post(OLLAMA_URL, payload, {
  timeout: OLLAMA_TIMEOUT  // Default: 120000ms (2 min)
})
```

**Escenarios:**
1. **Hardware lento**: CPU d√©bil, sin GPU
2. **TOP_K muy alto**: Muchos chunks ‚Üí mucho contexto ‚Üí LLM lento
3. **Modelo grande**: mixtral en CPU

**Soluciones:**

**1. Aumentar timeout:**
```env
OLLAMA_TIMEOUT=300000  # 5 minutos
```

**2. Reducir contexto:**
```env
RAG_TOP_K=3            # Menos chunks
RAG_CHUNK_SIZE=600     # Chunks m√°s peque√±os
```

**3. Cambiar a modelo m√°s r√°pido:**
```bash
ollama pull phi3
OLLAMA_LLM_MODEL=phi3
```

**4. Verificar que Ollama est√© corriendo:**
```bash
curl http://localhost:11434/api/tags
# Si no responde: ollama serve
```

---

### Error: "No encontr√© esta informaci√≥n en los documentos"

**Causa ra√≠z:**
```javascript
// ragService.js:29-31
if (topDocs.length === 0) {
  throw new Error('No encontr√© documentos relevantes...');
}
```

**Posibles razones:**

**1. Umbral de similitud muy alto:**
```env
RAG_SIMILARITY_THRESHOLD=0.7  # Demasiado restrictivo
# Cambiar a:
RAG_SIMILARITY_THRESHOLD=0.2
```

**2. Chunks mal cortados:**
```bash
# Ver chunks de un documento
sqlite3 data/vectors.db "SELECT text FROM documents WHERE filename='archivo.pdf' LIMIT 5;"
```

**3. Embedding model no adecuado:**
```bash
# Probar con modelo m√°s grande
ollama pull mxbai-embed-large
OLLAMA_EMBED_MODEL=mxbai-embed-large
# Reprocesar PDFs
```

**4. Pregunta en diferente idioma:**
```javascript
// Los embeddings son sensibles al idioma
// PDF en ingl√©s + pregunta en espa√±ol = baja similitud
```

**Debug de similitudes:**
```javascript
// Agregar logging en ragService.js:27-33
console.log('Top docs:', topDocs.map(d => ({
  similarity: d.similarity,
  text: d.text.substring(0, 100)
})));
```

---

### Las respuestas son "creativas" (inventa informaci√≥n)

**Causa:**
```javascript
// prompts/ragPrompts.js
// Strict mode desactivado permite al LLM usar su conocimiento
```

**Soluci√≥n:**
```env
RAG_STRICT_MODE=true  # ‚Üê DEBE estar en true
```

**Verificar configuraci√≥n:**
```bash
curl http://localhost:3000/api/config
# Debe retornar: { strictMode: true, ... }
```

**Prompt en modo estricto:**
```javascript
// prompts/ragPrompts.js
if (strict) {
  prompt += `
IMPORTANTE: Solo puedes responder bas√°ndote en el contexto anterior.
Si la informaci√≥n no est√° en el contexto, responde exactamente:
"No encontr√© esta informaci√≥n en los documentos disponibles."

NO inventes, NO uses tu conocimiento general, SOLO usa el contexto.
  `;
}
```

---

## üìà M√©tricas y Monitoreo

### ¬øC√≥mo ver estad√≠sticas de los documentos?

**Respuesta:**

**Endpoint disponible:**
```bash
GET /api/documents
Authorization: Cookie sessionId=xxx

Response:
{
  "documents": [
    {"filename": "manual.pdf", "chunks": 87},
    {"filename": "guia.pdf", "chunks": 43}
  ],
  "total_chunks": 130
}
```

**C√≥digo (vectorStore.js:238-259):**
```javascript
getDocumentStats() {
  const stats = this.db.prepare(`
    SELECT filename, COUNT(*) as count
    FROM documents
    GROUP BY filename
  `).all();

  const total = this.db.prepare(
    'SELECT COUNT(*) as total FROM documents'
  ).get().total;

  return {
    totalDocuments: total,
    files: { 'archivo.pdf': 50, ... }
  };
}
```

**Queries SQL √∫tiles:**
```bash
# Tama√±o de la base de datos
du -h data/vectors.db

# Documentos procesados
sqlite3 data/vectors.db "SELECT DISTINCT filename FROM documents;"

# Chunks por documento
sqlite3 data/vectors.db "
  SELECT filename, COUNT(*) as chunks
  FROM documents
  GROUP BY filename
  ORDER BY chunks DESC;
"

# Total de chunks
sqlite3 data/vectors.db "SELECT COUNT(*) FROM documents;"

# Tama√±o promedio de texto por chunk
sqlite3 data/vectors.db "SELECT AVG(LENGTH(text)) FROM documents;"
```

---

### ¬øC√≥mo monitorear el rendimiento en producci√≥n?

**Respuesta:**

**Actualmente el c√≥digo tiene logs b√°sicos:**
```javascript
// documentProcessor.js:20-35
console.log(`üìÑ Procesando PDF: ${filename}`);
console.log(`‚úì Texto extra√≠do: ${length} caracteres`);
console.log(`‚úì Dividido en ${chunks.length} chunks`);
process.stdout.write(`\r‚è≥ Generando embeddings... ${i}/${total}`);
```

**Mejoras recomendadas:**

**1. Agregar middleware de timing:**
```javascript
// middleware/performance.js
app.use((req, res, next) => {
  const start = Date.now();
  res.on('finish', () => {
    const duration = Date.now() - start;
    console.log(`${req.method} ${req.path} - ${duration}ms`);
  });
  next();
});
```

**2. Logging estructurado (Winston):**
```javascript
const winston = require('winston');
const logger = winston.createLogger({
  format: winston.format.json(),
  transports: [
    new winston.transports.File({ filename: 'rag.log' })
  ]
});

logger.info('Query executed', {
  query: query,
  resultsCount: topDocs.length,
  avgSimilarity: avg,
  duration: duration
});
```

**3. M√©tricas con Prometheus:**
```javascript
const client = require('prom-client');

const queryDuration = new client.Histogram({
  name: 'rag_query_duration_seconds',
  help: 'Duration of RAG queries'
});

const queriesTotal = new client.Counter({
  name: 'rag_queries_total',
  help: 'Total number of queries'
});
```

---

## üîß Arquitectura y C√≥digo

### ¬øPor qu√© separar los servicios en m√≥dulos?

**Respuesta:**

**Estructura modular implementada:**
```
services/
‚îú‚îÄ‚îÄ authService.js         # Autenticaci√≥n y sesiones
‚îú‚îÄ‚îÄ chunking.js            # L√≥gica de divisi√≥n de texto
‚îú‚îÄ‚îÄ documentProcessor.js   # Pipeline de procesamiento
‚îú‚îÄ‚îÄ embeddingService.js    # Comunicaci√≥n con Ollama
‚îú‚îÄ‚îÄ pdfProcessor.js        # Extracci√≥n de PDF
‚îú‚îÄ‚îÄ ragService.js          # L√≥gica RAG
‚îú‚îÄ‚îÄ responseFormatter.js   # Formato de respuestas
‚îú‚îÄ‚îÄ userDatabase.js        # CRUD de usuarios
‚îî‚îÄ‚îÄ vectorStore.js         # Almacenamiento vectorial
```

**Principios aplicados:**

**1. Separation of Concerns**
```javascript
// server.js solo maneja routing
app.post('/api/query', requireAuth, async (req, res) => {
  const topDocs = await performRAGSearch(...);  // ‚Üê ragService
  const response = await generateRAGResponse(...); // ‚Üê ragService
});
```

**2. Single Responsibility**
```javascript
// vectorStore.js solo maneja persistencia
class VectorStore {
  insertChunk() { ... }
  searchSimilar() { ... }
  // NO tiene l√≥gica de embeddings o PDFs
}
```

**3. Dependency Injection**
```javascript
// server.js
const vectorStore = new VectorStore(dbPath);
const authService = new AuthService(usersDbPath);

// Se inyectan en middleware
app.post('/api/query', requireAuth(authService), ...);
```

**4. Testabilidad**
```javascript
// F√°cil de testear en aislamiento
const { splitIntoSemanticChunks } = require('./chunking');

test('should split text into chunks', () => {
  const chunks = splitIntoSemanticChunks('Hello. World.', 10);
  expect(chunks).toHaveLength(2);
});
```

---

### ¬øC√≥mo funciona el streaming de respuestas?

**Respuesta:**

**Tecnolog√≠a: Server-Sent Events (SSE)**

**1. Cliente abre conexi√≥n persistente:**
```javascript
// Frontend (public/chat.html o similar)
const eventSource = new EventSource('/api/query-stream', {
  method: 'POST',
  body: JSON.stringify({ query: '...' })
});

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.type === 'token') {
    appendToChat(data.content);  // Agrega token en tiempo real
  }
};
```

**2. Servidor configura SSE:**
```javascript
// server.js:348-351
res.setHeader('Content-Type', 'text/event-stream');
res.setHeader('Cache-Control', 'no-cache');
res.setHeader('Connection', 'keep-alive');
res.flushHeaders();
```

**3. Ollama genera streaming:**
```javascript
// embeddingService.js
axios.post('http://localhost:11434/api/generate', {
  model: 'mistral',
  prompt: prompt,
  stream: true  // ‚Üê Clave para streaming
}, {
  responseType: 'stream'
});

return response.data;  // Retorna ReadableStream
```

**4. Relay de tokens:**
```javascript
// ragService.js:88-116
response.data.on('data', (chunk) => {
  const lines = chunk.toString().split('\n');

  for (const line of lines) {
    const json = JSON.parse(line);

    if (json.response) {
      // Enviar token inmediatamente al cliente
      res.write(`data: ${JSON.stringify({
        type: 'token',
        content: json.response
      })}\n\n`);
    }

    if (json.done) {
      res.write('data: [DONE]\n\n');
      res.end();
    }
  }
});
```

**Flujo completo:**
```
Usuario ‚Üí POST /api/query-stream
    ‚Üì
Server abre SSE
    ‚Üì
Server ‚Üí Ollama (stream: true)
    ‚Üì
Ollama genera token "La" ‚Üí Server ‚Üí Cliente (actualiza UI)
    ‚Üì
Ollama genera token " respuesta" ‚Üí Server ‚Üí Cliente
    ‚Üì
... (contin√∫a token por token)
    ‚Üì
Ollama genera done: true ‚Üí Server cierra stream
```

**Ventaja:** UX superior, usuario no espera 15 segundos en blanco

---

## üéØ Comparaci√≥n con Otras Soluciones

### ¬øC√≥mo se compara con LangChain?

| Aspecto | Este Proyecto | LangChain |
|---------|---------------|-----------|
| **Complejidad** | ‚úÖ Simple, ~2000 l√≠neas | ‚ö†Ô∏è Complejo, muchas abstracciones |
| **Dependencias** | ‚úÖ 9 packages | ‚ö†Ô∏è 50+ packages |
| **Curva aprendizaje** | ‚úÖ C√≥digo directo | ‚ö†Ô∏è API compleja |
| **Flexibilidad** | ‚úÖ Control total | ‚ö†Ô∏è Limitado a abstracciones |
| **Performance** | ‚úÖ Sin overhead | ‚ö†Ô∏è Capas extra de abstracci√≥n |
| **Debugging** | ‚úÖ Stack traces claros | ‚ö†Ô∏è Dif√≠cil seguir el flujo |
| **Features** | ‚ö†Ô∏è RAG b√°sico | ‚úÖ Agentes, memory, tools, etc |

**Conclusi√≥n**: Este proyecto es ideal para aprender RAG desde cero y tener control total. LangChain es mejor para prototipos complejos r√°pidos.

---

### ¬øC√≥mo se compara con Pinecone + OpenAI?

| Aspecto | SQLite + Ollama | Pinecone + OpenAI |
|---------|----------------|-------------------|
| **Privacidad** | ‚úÖ 100% local | ‚ùå Datos en la nube |
| **Costo mensual** | ‚úÖ $0 | ‚ö†Ô∏è ~$70-200/mes |
| **Setup** | ‚úÖ npm install | ‚ö†Ô∏è Cuentas, APIs, billing |
| **Escalabilidad** | ‚ö†Ô∏è ~10K docs | ‚úÖ Millones de docs |
| **Latencia query** | ‚ö†Ô∏è 6-18 seg | ‚úÖ 1-3 seg |
| **Calidad embeddings** | ‚ö†Ô∏è Buena | ‚úÖ Excelente |
| **Calidad respuestas** | ‚ö†Ô∏è Buena | ‚úÖ GPT-4 nivel |
| **Offline** | ‚úÖ Funciona sin internet | ‚ùå Requiere internet |

**Recomendaci√≥n:**
- **Local**: Documentos sensibles, presupuesto ajustado, <10K docs
- **Cloud**: Producci√≥n a escala, mejor calidad, SLA garantizado

---

## ‚úÖ Conclusi√≥n

Este documento cubre las preguntas t√©cnicas m√°s comunes. Para preguntas adicionales:

- **C√≥digo fuente**: Revisar comentarios en los archivos
- **Logs**: Ejecutar con `NODE_ENV=development` para ver logs detallados
- **Debugging**: Usar `console.log` en cualquier servicio
- **Issues**: Reportar en el repositorio

**Contacto del proyecto:**
- Repositorio: https://github.com/alejandro-urbina-rasi/Chat-RAG-Local
